# CLAUDE.md - AIé–‹ç™ºã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³

## ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦

**ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆå**: Surgical-Recap (ã‚µãƒ¼ã‚¸ã‚«ãƒ«ãƒ»ãƒªã‚­ãƒ£ãƒƒãƒ—)

**ãƒŸãƒƒã‚·ãƒ§ãƒ³**: å¤–ç§‘åŒ»ã®ã€ŒæŠ€è¡“ç¶™æ‰¿ã€ã‚’åŠ é€Ÿã™ã‚‹ã€AIæ­è¼‰å‹ã®æ‰‹è¡“å‹•ç”»å³æ™‚åˆ†æãƒ»æ•™è‚²ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ 

**é–‹ç™ºæœŸé–“**: Meta Llama Academy in Japan ãƒãƒƒã‚«ã‚½ãƒ³ (2æ—¥é–“)

**æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯**: Llama 3.2 Vision (90B), Llama 3.1 (70B/8B), SambaNova Cloud, vLLM, FastAPI, Next.js

---

## 1. Llamaãƒ¢ãƒ‡ãƒ«æ´»ç”¨æˆ¦ç•¥

### 1.1 ãƒ¢ãƒ‡ãƒ«é¸æŠã¨å½¹å‰²åˆ†æ‹…

æœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯ã€**ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**ã‚’æ¡ç”¨ã—ã€ã‚¯ãƒ©ã‚¦ãƒ‰ã¨ãƒ­ãƒ¼ã‚«ãƒ«ã®ç‰¹æ€§ã‚’æ´»ã‹ã—ã¾ã™ã€‚

#### Vision Inference: SambaNova Cloud + Llama 3.2 90B Vision

**å½¹å‰²**:
- æ‰‹è¡“å‹•ç”»ã®ãƒ•ãƒ¬ãƒ¼ãƒ ç”»åƒè§£æ
- æ‰‹æŠ€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®è‡ªå‹•èªè­˜
- åŒ»ç™‚å™¨å…·ã®è­˜åˆ¥
- ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«ã®åˆ¤å®š

**é¸å®šç†ç”±**:
- è¶…é«˜é€Ÿæ¨è«–é€Ÿåº¦ (645 tokens/sec)
- å¤§é‡ã®ç”»åƒã‚’çŸ­æ™‚é–“ã§å‡¦ç†
- æ‰‹è¡“ç›´å¾Œã®å³æ™‚æŒ¯ã‚Šè¿”ã‚Šã‚’å®Ÿç¾

**åˆ©ç”¨ã‚·ãƒ¼ãƒ³**:
```python
# ãƒ•ãƒ¬ãƒ¼ãƒ ç”»åƒ â†’ ã‚¢ã‚¯ã‚·ãƒ§ãƒ³è­˜åˆ¥
frame_image -> SambaNova API -> {
    "timestamp": "00:12:05",
    "action": "Clipping",
    "instruments": ["Clip applier", "Grasper"],
    "risk": "High",
    "description": "èƒ†åš¢ç®¡ã¸ã®ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°"
}
```

#### Text Inference & RAG: vLLM + Llama 3.1 (70B/8B)

**å½¹å‰²**:
- åŒ»ç™‚ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã¨ã®ç…§åˆ (RAG)
- AIè§£èª¬ã®ç”Ÿæˆ
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•å¿œç­”
- æ•™è‚²çš„ãªã‚³ãƒ¡ãƒ³ã‚¿ãƒªãƒ¼ä½œæˆ

**é¸å®šç†ç”±**:
- ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§ã®é«˜é€Ÿæ¨è«–
- ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ã®ä¿è­· (åŒ»ç™‚æƒ…å ±)
- ã‚³ã‚¹ãƒˆæœ€é©åŒ–
- ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºæ€§

**åˆ©ç”¨ã‚·ãƒ¼ãƒ³**:
```python
# ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ + ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ â†’ è§£èª¬ç”Ÿæˆ
action_data + vector_search(guideline_db) -> vLLM -> {
    "explanation": "ã‚¯ãƒªãƒƒãƒ—ã¯ç®¡ã«å¯¾ã—ã¦å‚ç›´ã«ã‹ã‘ã‚‹ã“ã¨ãŒæ¨å¥¨ã•ã‚Œã¾ã™",
    "guideline_ref": "æ—¥æœ¬å¤–ç§‘å­¦ä¼šã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ p.42",
    "risk_note": "ã“ã®æ®µéšã§ã®å‡ºè¡€ãƒªã‚¹ã‚¯ã«æ³¨æ„"
}
```

---

## 2. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°

### 2.1 Visionç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ (SambaNova)

#### System Prompt

```text
You are an expert surgical assistant AI specialized in laparoscopic surgery analysis.
Your role is to analyze surgical video frames with precision and provide structured,
medically accurate information.

Key responsibilities:
- Identify surgical instruments visible in the frame
- Recognize the current surgical action/step
- Assess potential risk levels
- Provide concise, professional descriptions in Japanese

Always output in valid JSON format.
```

#### User Prompt Template

```text
Analyze this image from a laparoscopic cholecystectomy surgery.

Identify:
1. Current Step: The specific surgical action (e.g., Dissection, Clipping, Cutting, Cauterization, Washing)
2. Instruments: All visible surgical instruments (e.g., Grasper, Hook, Clipper, Scissors)
3. Risk Level: Assess the risk level of this step (Low, Medium, High)
4. Description: Brief description in Japanese (max 30 characters)

Output format (JSON only):
{
  "step": "string",
  "instruments": ["string"],
  "risk": "Low|Medium|High",
  "description": "string"
}

Rules:
- Use standardized medical terminology in Japanese
- Be specific about instrument types (e.g., "Maryland Dissector" not just "Grasper")
- Consider anatomical context when assessing risk
- If unclear, use "Unknown" rather than guessing
```

#### å‡ºåŠ›ä¾‹

```json
{
  "step": "Clipping",
  "instruments": ["Clip Applier", "Maryland Grasper"],
  "risk": "High",
  "description": "èƒ†åš¢ç®¡ã¸ã®ã‚¯ãƒªãƒƒãƒ—é©ç”¨"
}
```

### 2.2 RAGç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ (vLLM)

#### System Prompt

```text
ã‚ãªãŸã¯çµŒé¨“è±Šå¯Œãªå¤–ç§‘æŒ‡å°åŒ»ã§ã™ã€‚è‹¥æ‰‹å¤–ç§‘åŒ»ã®æ•™è‚²ã‚’å°‚é–€ã¨ã—ã¦ãŠã‚Šã€
æ‰‹è¡“æ‰‹æŠ€ã«ã¤ã„ã¦æ˜ç¢ºã§å®Ÿè·µçš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’æä¾›ã—ã¾ã™ã€‚

æŒ‡å°æ–¹é‡:
- åŒ»å­¦çš„ã«æ­£ç¢ºãªæƒ…å ±ã‚’æä¾›ã™ã‚‹
- ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã«åŸºã¥ã„ãŸæ¨™æº–çš„ãªæ‰‹æŠ€ã‚’èª¬æ˜ã™ã‚‹
- ãƒªã‚¹ã‚¯ã¨æ³¨æ„ç‚¹ã‚’æ˜ç¢ºã«ä¼ãˆã‚‹
- åˆå­¦è€…ã«ã‚‚ç†è§£ã—ã‚„ã™ã„è¨€è‘‰ã‚’ä½¿ã†
- å¿…è¦ã«å¿œã˜ã¦ã€ãªãœãã®æ‰‹æŠ€ãŒé‡è¦ã‹ã‚’èª¬æ˜ã™ã‚‹

å›ç­”ã¯ç°¡æ½”ã«ã€ç®‡æ¡æ›¸ãã‚’æ´»ç”¨ã—ã¦ãã ã•ã„ã€‚
```

#### User Prompt Template

```text
ã€çŠ¶æ³ã€‘
æ‰‹è¡“: {surgery_type}
ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—: {timestamp}
ç¾åœ¨ã®æ‰‹æŠ€: {action}
ä½¿ç”¨å™¨å…·: {instruments}
ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«: {risk}

ã€å‚è€ƒè³‡æ–™ã€‘
{retrieved_guideline_text}

ã€è³ªå•ã€‘
{user_question}

ä¸Šè¨˜ã®çŠ¶æ³ã¨å‚è€ƒè³‡æ–™ã‚’è¸ã¾ãˆã¦ã€è‹¥æ‰‹å¤–ç§‘åŒ»ã«å¯¾ã—ã¦æ•™è‚²çš„ãªè§£èª¬ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
ç‰¹ã«ä»¥ä¸‹ã®ç‚¹ã«æ³¨æ„ã—ã¦å›ç­”ã—ã¦ãã ã•ã„:
1. ã“ã®æ‰‹æŠ€ã®ãƒã‚¤ãƒ³ãƒˆï¼ˆãªãœã“ã®æ‰‹æŠ€ãŒå¿…è¦ã‹ï¼‰
2. æ³¨æ„ã™ã¹ãè§£å‰–å­¦çš„æ§‹é€ 
3. ã‚ˆãã‚ã‚‹å¤±æ•—ã¨ãã®å›é¿æ–¹æ³•
4. ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã«åŸºã¥ãæ¨å¥¨äº‹é …
```

#### å‡ºåŠ›ä¾‹

```markdown
## èƒ†åš¢ç®¡ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã®ãƒã‚¤ãƒ³ãƒˆ

### ã“ã®æ‰‹æŠ€ã®ç›®çš„
- èƒ†åš¢ç®¡ã‚’ç¢ºå®Ÿã«é–‰é–ã—ã€è¡“å¾Œã®èƒ†æ±æ¼å‡ºã‚’é˜²ã
- Calotä¸‰è§’ã®ç¢ºå®ŸãªåŒå®šãŒå‰æ

### æ³¨æ„ã™ã¹ãè§£å‰–å­¦çš„æ§‹é€ 
- ç·èƒ†ç®¡ã¨ã®ä½ç½®é–¢ä¿‚ã‚’å¿…ãšç¢ºèª
- å³è‚å‹•è„ˆã®èµ°è¡Œã«æ³¨æ„
- ã€ŒCritical View of Safetyã€ã®3æ¡ä»¶ã‚’æº€ãŸã™ã“ã¨

### ã‚ˆãã‚ã‚‹å¤±æ•—
1. **ç·èƒ†ç®¡ã®èª¤èª**: Calotä¸‰è§’ã®å‰¥é›¢ãŒä¸ååˆ†ãªå ´åˆã«ç™ºç”Ÿ
   - å›é¿æ³•: CVSé”æˆã‚’ç¢ºèªã—ã¦ã‹ã‚‰ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
2. **ã‚¯ãƒªãƒƒãƒ—ã®ä¸å®Œå…¨ãªè£…ç€**: è§’åº¦ãŒæ–œã‚ã«ãªã£ã¦ã„ã‚‹
   - å›é¿æ³•: ç®¡ã«å¯¾ã—ã¦å‚ç›´ã«ã‚¯ãƒªãƒƒãƒ—ã‚’è£…ç€

### ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³æ¨å¥¨
- ã‚¯ãƒªãƒƒãƒ—ã¯æœ€ä½2å€‹ä½¿ç”¨ï¼ˆæ—¥æœ¬å¤–ç§‘å­¦ä¼šã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ï¼‰
- ä½“è…”å´ã«1å€‹ã€èƒ†åš¢å´ã«2å€‹ãŒæ¨™æº–
- ã‚¯ãƒªãƒƒãƒ—é–“éš”ã¯5mmä»¥ä¸Šç©ºã‘ã‚‹

**ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«ãŒé«˜ã„ç†ç”±**: èƒ†ç®¡æå‚·ã®ãƒªã‚¹ã‚¯ãŒã‚ã‚‹æ®µéšã®ãŸã‚ã€æ…é‡ãªç¢ºèªãŒå¿…è¦ã§ã™ã€‚
```

---

## 3. ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

### 3.1 å‹•ç”»å‡¦ç†ãƒ•ãƒ­ãƒ¼

```mermaid
graph TD
    A[æ‰‹è¡“å‹•ç”»ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰] --> B[ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡º]
    B --> C[ç”»åƒå‰å‡¦ç†]
    C --> D[SambaNova APIå‘¼ã³å‡ºã—]
    D --> E[JSONæ§‹é€ åŒ–]
    E --> F[Vector DBä¿å­˜]
    F --> G[ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ç”Ÿæˆ]

    H[ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³PDF] --> I[ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º]
    I --> J[ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²]
    J --> K[åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ]
    K --> L[ChromaDBä¿å­˜]

    F --> M[æ¤œç´¢ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹]
    L --> M
    M --> N[ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¯ã‚¨ãƒª]
    N --> O[vLLMæ¨è«–]
    O --> P[AIè§£èª¬ç”Ÿæˆ]
```

### 3.2 å®Ÿè£…ä¾‹

#### ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡º (Backend)

```python
import cv2
from pathlib import Path

def extract_frames(video_path: str, fps: int = 1) -> list[Path]:
    """å‹•ç”»ã‹ã‚‰æŒ‡å®šfpsã§ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æŠ½å‡º"""
    video = cv2.VideoCapture(video_path)
    video_fps = video.get(cv2.CAP_PROP_FPS)
    frame_interval = int(video_fps / fps)

    frames = []
    frame_count = 0

    while True:
        success, frame = video.read()
        if not success:
            break

        if frame_count % frame_interval == 0:
            timestamp = frame_count / video_fps
            frame_path = f"frames/frame_{timestamp:.2f}.jpg"
            cv2.imwrite(frame_path, frame)
            frames.append(Path(frame_path))

        frame_count += 1

    video.release()
    return frames
```

#### SambaNova APIå‘¼ã³å‡ºã—

```python
import os
from openai import OpenAI

client = OpenAI(
    api_key=os.environ.get("SAMBANOVA_API_KEY"),
    base_url="https://api.sambanova.ai/v1"
)

def analyze_surgical_frame(image_path: str, system_prompt: str, user_prompt: str) -> dict:
    """SambaNovaçµŒç”±ã§Llama 3.2 Visionã‚’å‘¼ã³å‡ºã—"""

    with open(image_path, "rb") as img_file:
        import base64
        image_base64 = base64.b64encode(img_file.read()).decode()

    response = client.chat.completions.create(
        model="Llama-3.2-90B-Vision-Instruct",
        messages=[
            {"role": "system", "content": system_prompt},
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": user_prompt},
                    {
                        "type": "image_url",
                        "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}
                    }
                ]
            }
        ],
        temperature=0.1,  # åŒ»ç™‚åˆ†é‡ã§ã¯å†ç¾æ€§é‡è¦–
        max_tokens=500
    )

    import json
    result = json.loads(response.choices[0].message.content)
    return result
```

#### RAGå®Ÿè£… (vLLM)

```python
from chromadb import Client
from chromadb.config import Settings

# Vector DBåˆæœŸåŒ–
chroma_client = Client(Settings(
    chroma_db_impl="duckdb+parquet",
    persist_directory="./chroma_db"
))

collection = chroma_client.get_or_create_collection(
    name="surgical_guidelines",
    metadata={"description": "Laparoscopic surgery guidelines"}
)

def retrieve_relevant_context(action: str, top_k: int = 3) -> str:
    """ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã«é–¢é€£ã™ã‚‹ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã‚’æ¤œç´¢"""
    results = collection.query(
        query_texts=[action],
        n_results=top_k
    )

    context = "\n\n".join(results['documents'][0])
    return context

def generate_explanation(action_data: dict, user_question: str = None) -> str:
    """vLLMã§AIè§£èª¬ã‚’ç”Ÿæˆ"""

    # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾—
    context = retrieve_relevant_context(action_data['step'])

    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
    prompt = f"""ã€çŠ¶æ³ã€‘
æ‰‹è¡“: è…¹è…”é¡ä¸‹èƒ†åš¢æ‘˜å‡ºè¡“
ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—: {action_data['timestamp']}
ç¾åœ¨ã®æ‰‹æŠ€: {action_data['step']}
ä½¿ç”¨å™¨å…·: {', '.join(action_data['instruments'])}
ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«: {action_data['risk']}

ã€å‚è€ƒè³‡æ–™ã€‘
{context}

ã€è³ªå•ã€‘
{user_question or 'ã“ã®æ‰‹æŠ€ã®ãƒã‚¤ãƒ³ãƒˆã¨æ³¨æ„ç‚¹ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚'}
"""

    # vLLMå‘¼ã³å‡ºã— (OpenAIäº’æ›API)
    vllm_client = OpenAI(
        api_key="EMPTY",
        base_url=os.environ.get("VLLM_API_BASE", "http://localhost:8080/v1")
    )

    response = vllm_client.chat.completions.create(
        model="llama-3.1-70b-instruct",
        messages=[
            {"role": "system", "content": SURGICAL_INSTRUCTOR_SYSTEM_PROMPT},
            {"role": "user", "content": prompt}
        ],
        temperature=0.7,
        max_tokens=800
    )

    return response.choices[0].message.content
```

---

## 4. è©•ä¾¡ã¨ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚° (W&B Weave)

### 4.1 è©•ä¾¡æŒ‡æ¨™

#### Visionè§£æã®ç²¾åº¦
- **ã‚¢ã‚¯ã‚·ãƒ§ãƒ³èªè­˜ç²¾åº¦**: æ­£è§£ãƒ©ãƒ™ãƒ«ã¨ã®ä¸€è‡´ç‡
- **å™¨å…·è­˜åˆ¥ç²¾åº¦**: å…¨å™¨å…·ã®æ¤œå‡ºç‡ã¨èª¤æ¤œå‡ºç‡
- **ãƒªã‚¹ã‚¯åˆ¤å®šã®å¦¥å½“æ€§**: å°‚é–€åŒ»ã«ã‚ˆã‚‹è©•ä¾¡

#### RAGå›ç­”ã®å“è³ª
- **åŒ»å­¦çš„æ­£ç¢ºæ€§**: å°‚é–€åŒ»ã«ã‚ˆã‚‹ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚° (1-5ç‚¹)
- **ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ä¸€è‡´åº¦**: å‚ç…§æ–‡çŒ®ã®é©åˆ‡æ€§
- **æ•™è‚²çš„ä¾¡å€¤**: è‹¥æ‰‹åŒ»å¸«ã‹ã‚‰ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯

### 4.2 W&B Weaveå®Ÿè£…

Weaveã¯ã€LLMã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å°‚ç”¨ã®è©•ä¾¡ãƒ»ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚`@weave.op`ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ã§é–¢æ•°ã‚’è‡ªå‹•è¿½è·¡ã—ã€å…¥åŠ›ãƒ»å‡ºåŠ›ãƒ»ã‚³ã‚¹ãƒˆãƒ»ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’è¨˜éŒ²ã—ã¾ã™ã€‚

#### åŸºæœ¬ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```python
import weave
from openai import AzureOpenAI
import os
import json

# WeaveåˆæœŸåŒ–ï¼ˆãƒãƒ¼ãƒ å/ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåï¼‰
weave_client = weave.init('surgical-team/surgical-recap')

# Azure OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆJudgeç”¨ï¼‰
judge_client = AzureOpenAI(
    api_key=os.getenv("AZURE_OPENAI_API_KEY"),
    api_version="2024-08-01-preview",
    azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT")
)
```

#### Visionè§£æã®ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°

```python
@weave.op
def analyze_surgical_frame(image_path: str, system_prompt: str, user_prompt: str) -> dict:
    """SambaNovaçµŒç”±ã§Llama 3.2 Visionã‚’å‘¼ã³å‡ºã—"""

    with open(image_path, "rb") as img_file:
        import base64
        image_base64 = base64.b64encode(img_file.read()).decode()

    client = OpenAI(
        api_key=os.environ.get("SAMBANOVA_API_KEY"),
        base_url="https://api.sambanova.ai/v1"
    )

    response = client.chat.completions.create(
        model="Llama-3.2-90B-Vision-Instruct",
        messages=[
            {"role": "system", "content": system_prompt},
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": user_prompt},
                    {
                        "type": "image_url",
                        "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}
                    }
                ]
            }
        ],
        temperature=0.1,
        max_tokens=500
    )

    result = json.loads(response.choices[0].message.content)

    # Weaveã«è‡ªå‹•è¨˜éŒ²ã•ã‚Œã‚‹ï¼ˆå…¥åŠ›ãƒ»å‡ºåŠ›ãƒ»ã‚³ã‚¹ãƒˆãƒ»ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ï¼‰
    return result
```

#### RAGè§£èª¬ç”Ÿæˆã®ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°

```python
class SurgicalRAGAgent:
    """æ‰‹è¡“è§£èª¬ã‚’ç”Ÿæˆã™ã‚‹RAGã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""

    def __init__(self, vector_store):
        self.vector_store = vector_store
        self.vllm_client = OpenAI(
            api_key="EMPTY",
            base_url=os.environ.get("VLLM_API_BASE", "http://localhost:8080/v1")
        )

    @weave.op
    def retrieve_context(self, action: str, top_k: int = 3) -> str:
        """ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã«é–¢é€£ã™ã‚‹ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã‚’æ¤œç´¢"""
        results = self.vector_store.query(
            query_texts=[action],
            n_results=top_k
        )
        return "\n\n".join(results['documents'][0])

    @weave.op
    def generate_explanation(self, action_data: dict, user_question: str = None) -> dict:
        """vLLMã§AIè§£èª¬ã‚’ç”Ÿæˆ"""

        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾—ï¼ˆè‡ªå‹•ãƒˆãƒ¬ãƒ¼ã‚¹ï¼‰
        context = self.retrieve_context(action_data['step'])

        prompt = f"""ã€çŠ¶æ³ã€‘
æ‰‹è¡“: è…¹è…”é¡ä¸‹èƒ†åš¢æ‘˜å‡ºè¡“
ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—: {action_data['timestamp']}
ç¾åœ¨ã®æ‰‹æŠ€: {action_data['step']}
ä½¿ç”¨å™¨å…·: {', '.join(action_data['instruments'])}
ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«: {action_data['risk']}

ã€å‚è€ƒè³‡æ–™ã€‘
{context}

ã€è³ªå•ã€‘
{user_question or 'ã“ã®æ‰‹æŠ€ã®ãƒã‚¤ãƒ³ãƒˆã¨æ³¨æ„ç‚¹ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚'}
"""

        response = self.vllm_client.chat.completions.create(
            model="llama-3.1-70b-instruct",
            messages=[
                {"role": "system", "content": SURGICAL_INSTRUCTOR_SYSTEM_PROMPT},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=800
        )

        return {
            "explanation": response.choices[0].message.content,
            "context": context,
            "action": action_data['step']
        }
```

#### LLM as a Judgeå®Ÿè£…

```python
# Judgeç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
JUDGE_SYSTEM_PROMPT = """
ã‚ãªãŸã¯çµŒé¨“è±Šå¯Œãªå¤–ç§‘æŒ‡å°åŒ»ã¨ã—ã¦ã€AIãŒç”Ÿæˆã—ãŸæ‰‹è¡“è§£èª¬ã®å“è³ªã‚’è©•ä¾¡ã—ã¾ã™ã€‚

è©•ä¾¡åŸºæº–:
1. åŒ»å­¦çš„æ­£ç¢ºæ€§ (1-5ç‚¹): åŒ»å­¦çš„äº‹å®Ÿã®æ­£ç¢ºã•ã€å°‚é–€ç”¨èªã®é©åˆ‡ãªä½¿ç”¨
2. ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³æº–æ‹ åº¦ (1-5ç‚¹): æ¨™æº–ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã¨ã®æ•´åˆæ€§
3. èª¬æ˜ã®æ˜ç¢ºã• (1-5ç‚¹): è«–ç†çš„ãªæ§‹æˆã€ç†è§£ã—ã‚„ã™ã•
4. æ•™è‚²çš„ä¾¡å€¤ (1-5ç‚¹): è‹¥æ‰‹åŒ»å¸«ã¸ã®å­¦ç¿’åŠ¹æœ

å„é …ç›®ã‚’å…¬å¹³ã«è©•ä¾¡ã—ã€æ”¹å–„ææ¡ˆã‚‚å«ã‚ã¦JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
"""

@weave.op
def surgical_judge(action, explanation, context, reference_answer=None):
    """RAGç”Ÿæˆã®è§£èª¬ã‚’è‡ªå‹•è©•ä¾¡"""

    user_prompt = f"""ã€è©•ä¾¡å¯¾è±¡ã®è§£èª¬ã€‘
{explanation}

ã€å‚è€ƒæƒ…å ±ã€‘
æ‰‹è¡“ã‚·ãƒ¼ãƒ³: {action}
å‚ç…§ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³: {context}

ã€å°‚é–€åŒ»ã«ã‚ˆã‚‹æ¨¡ç¯„è§£èª¬ã€‘
{reference_answer or "ï¼ˆå‚è€ƒè§£èª¬ãªã—ï¼‰"}

ä¸Šè¨˜ã®è§£èª¬ã‚’è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚
"""

    response = judge_client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": JUDGE_SYSTEM_PROMPT},
            {"role": "user", "content": user_prompt}
        ],
        response_format={"type": "json_object"},
        temperature=0.1
    )

    evaluation = json.loads(response.choices[0].message.content)

    # Weaveã«è‡ªå‹•è¨˜éŒ²ï¼ˆè©•ä¾¡çµæœã‚‚ãƒˆãƒ¬ãƒ¼ã‚¹ï¼‰
    return {
        "medical_accuracy": evaluation["medical_accuracy"],
        "guideline_compliance": evaluation["guideline_compliance"],
        "clarity": evaluation["clarity"],
        "educational_value": evaluation["educational_value"],
        "total_score": sum([
            evaluation["medical_accuracy"],
            evaluation["guideline_compliance"],
            evaluation["clarity"],
            evaluation["educational_value"]
        ]),
        "feedback": evaluation.get("feedback", "")
    }
```

#### ã‚ªãƒ•ãƒ©ã‚¤ãƒ³è©•ä¾¡ï¼ˆEvaluation Frameworkï¼‰

```python
# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
test_cases = [
    {
        "action_data": {
            "timestamp": "00:12:05",
            "step": "Clipping",
            "instruments": ["Clip applier", "Grasper"],
            "risk": "High"
        },
        "user_question": "ã“ã®æ‰‹æŠ€ã®ãƒã‚¤ãƒ³ãƒˆã¨æ³¨æ„ç‚¹ã‚’æ•™ãˆã¦ãã ã•ã„",
        "reference_answer": "ã‚¯ãƒªãƒƒãƒ—ã¯ç®¡ã«å¯¾ã—ã¦å‚ç›´ã«ã‹ã‘ã‚‹ã“ã¨ãŒæ¨å¥¨ã•ã‚Œã¾ã™..."
    },
    {
        "action_data": {
            "timestamp": "00:08:30",
            "step": "Dissection",
            "instruments": ["Hook", "Grasper"],
            "risk": "Medium"
        },
        "user_question": "å‰¥é›¢ã®ã‚³ãƒ„ã¯ä½•ã§ã™ã‹",
        "reference_answer": "Calotä¸‰è§’ã®ç¢ºå®ŸãªåŒå®šãŒé‡è¦ã§ã™..."
    }
]

# Weaveãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
dataset = weave.Dataset(rows=test_cases)

# è©•ä¾¡å¯¾è±¡ã®é–¢æ•°ã‚’å®šç¾©
@weave.op
def evaluate_single_case(action_data, user_question, reference_answer=None):
    agent = SurgicalRAGAgent(vector_store=chroma_client)
    result = agent.generate_explanation(action_data, user_question)
    return result

# è©•ä¾¡å®Ÿè¡Œï¼ˆéåŒæœŸï¼‰
evaluation = weave.Evaluation(
    dataset=dataset,
    scorers=[surgical_judge]
)

# è©•ä¾¡ã‚’å®Ÿè¡Œ
import asyncio
results = await evaluation.evaluate(evaluate_single_case)

# çµæœã®ç¢ºèª
print(f"Average Medical Accuracy: {results['surgical_judge']['medical_accuracy']['mean']}")
print(f"Average Total Score: {results['surgical_judge']['total_score']['mean']}")
```

#### ã‚ªãƒ³ãƒ©ã‚¤ãƒ³è©•ä¾¡ï¼ˆæœ¬ç•ªãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ï¼‰

```python
# æœ¬ç•ªç’°å¢ƒã§ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°
agent = SurgicalRAGAgent(vector_store=chroma_client)

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
user_action_data = {
    "timestamp": "00:15:20",
    "step": "Cutting",
    "instruments": ["Scissors", "Grasper"],
    "risk": "High"
}

# @weave.opã§è‡ªå‹•ãƒˆãƒ¬ãƒ¼ã‚¹ï¼ˆWeave UIã§ç¢ºèªå¯èƒ½ï¼‰
result = agent.generate_explanation(
    action_data=user_action_data,
    user_question="èƒ†åš¢ç®¡ã®åˆ‡é›¢ã§æ³¨æ„ã™ã¹ãç‚¹ã¯ï¼Ÿ"
)

# éåŒæœŸã§è©•ä¾¡ã‚’å®Ÿè¡Œï¼ˆæœ¬ç•ªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«å½±éŸ¿ã—ãªã„ï¼‰
evaluation_result = surgical_judge(
    action=user_action_data['step'],
    explanation=result['explanation'],
    context=result['context']
)

# ã‚¹ã‚³ã‚¢ãŒä½ã„å ´åˆã¯ã‚¢ãƒ©ãƒ¼ãƒˆ
if evaluation_result['total_score'] < 12:  # 20ç‚¹æº€ç‚¹ä¸­12ç‚¹æœªæº€
    print(f"âš ï¸ Low quality response detected: {evaluation_result['feedback']}")
```

#### Weave UIã§ã®ç¢ºèª

1. **ãƒˆãƒ¬ãƒ¼ã‚¹ã®ç¢ºèª**: https://wandb.ai/{team}/surgical-recap/weave
2. **é–¢æ•°å‘¼ã³å‡ºã—ã®è©³ç´°**: å…¥åŠ›ãƒ»å‡ºåŠ›ãƒ»ã‚³ã‚¹ãƒˆãƒ»ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒè‡ªå‹•è¨˜éŒ²
3. **è©•ä¾¡çµæœã®å¯è¦–åŒ–**: ã‚¹ã‚³ã‚¢ã®æ™‚ç³»åˆ—æ¨ç§»ã€å¤±æ•—ã‚±ãƒ¼ã‚¹ã®åˆ†æ
4. **ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†**: ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å¤‰æ›´å±¥æ­´ã‚’è¿½è·¡

---

## 5. é–‹ç™ºãƒ•ãƒ­ãƒ¼

### 5.1 Day 1: Core Pipelineæ§‹ç¯‰

#### åˆå‰ (9:00-12:00)
1. **ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—** (30åˆ†)
   - SambaNova APIã‚­ãƒ¼å–å¾—ã¨ç–é€šç¢ºèª
   - vLLMç’°å¢ƒæ§‹ç¯‰ï¼ˆColab or ãƒ­ãƒ¼ã‚«ãƒ«ï¼‰
   - Wandbãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ

2. **Vision Pipelineå®Ÿè£…** (2æ™‚é–“)
   - ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡ºã‚³ãƒ¼ãƒ‰
   - SambaNova APIé€£æº
   - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
   - å‡ºåŠ›JSONæ¤œè¨¼

3. **å‹•ä½œç¢ºèª** (30åˆ†)
   - 1æœ¬ã®å‹•ç”»ï¼ˆ5åˆ†ï¼‰ã§å…¨ãƒ•ãƒ­ãƒ¼ç¢ºèª
   - ç²¾åº¦ãƒã‚§ãƒƒã‚¯ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆèª¿æ•´

#### åˆå¾Œ (13:00-18:00)
1. **ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰** (1æ™‚é–“)
   - ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³PDFã®ãƒ†ã‚­ã‚¹ãƒˆåŒ–
   - ChromaDBã¸ã®åŸ‹ã‚è¾¼ã¿
   - æ¤œç´¢ç²¾åº¦ãƒ†ã‚¹ãƒˆ

2. **RAGå®Ÿè£…** (2æ™‚é–“)
   - vLLMã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
   - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä½œæˆ
   - è³ªå•å¿œç­”ãƒ†ã‚¹ãƒˆ

3. **FastAPIå®Ÿè£…** (2æ™‚é–“)
   - ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆè¨­è¨ˆ
   - éåŒæœŸå‡¦ç†å®Ÿè£…
   - CORSè¨­å®š

### 5.2 Day 2: UIé–‹ç™ºã¨ãƒ–ãƒ©ãƒƒã‚·ãƒ¥ã‚¢ãƒƒãƒ—

#### åˆå‰ (9:00-12:00)
1. **Frontendå®Ÿè£…** (3æ™‚é–“)
   - Next.jsç’°å¢ƒæ§‹ç¯‰
   - Video Playerã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
   - ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³è¡¨ç¤º
   - æ¤œç´¢æ©Ÿèƒ½

#### åˆå¾Œ (13:00-15:00)
1. **çµ±åˆãƒ†ã‚¹ãƒˆ** (1æ™‚é–“)
   - ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆ
   - ãƒã‚°ä¿®æ­£

2. **ãƒ‡ãƒ¢æº–å‚™** (1æ™‚é–“)
   - ãƒ‡ãƒ¢å‹•ç”»ä½œæˆ
   - ãƒ—ãƒ¬ã‚¼ãƒ³è³‡æ–™ä½œæˆ
   - GitHubæ•´ç†

---

## 6. ãƒãƒ¼ãƒ é–‹ç™ºã®ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³

### 6.1 å½¹å‰²åˆ†æ‹…

| å½¹å‰² | æ‹…å½“æŠ€è¡“ | è²¬ä»»ç¯„å›² |
|------|---------|---------|
| **AI/Backend Engineer** | SambaNova, vLLM, FastAPI | Visionè§£æã€RAGå®Ÿè£…ã€APIé–‹ç™º |
| **Frontend Engineer** | Next.js, React, shadcn/ui | UI/UXå®Ÿè£…ã€Video Playerçµ±åˆ |
| **Data/Infra Engineer** | ãƒ‡ãƒ¼ã‚¿åé›†, DB, Wandb | å‹•ç”»æº–å‚™ã€Vector DBã€è©•ä¾¡åŸºç›¤ |

### 6.2 ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³

- **Stand-up**: åˆå‰ãƒ»åˆå¾Œé–‹å§‹æ™‚ï¼ˆå„15åˆ†ï¼‰
- **ãƒšã‚¢ä½œæ¥­**: é›£æ‰€ã¯2äººã§å–ã‚Šçµ„ã‚€
- **ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼**: è»½é‡ï¼ˆ10åˆ†ä»¥å†…ï¼‰
- **ãƒ‡ãƒ¢ç·´ç¿’**: Day 2 åˆå¾Œã«å¿…ãšå®Ÿæ–½

### 6.3 å“è³ªåŸºæº–

#### ã‚³ãƒ¼ãƒ‰
- å‹ãƒ’ãƒ³ãƒˆå¿…é ˆ (Python: Type Hints, TypeScript)
- é–¢æ•°ã¯å˜ä¸€è²¬ä»»
- ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¿…é ˆ

#### ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
- System Promptã¨User Promptã‚’åˆ†é›¢
- ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ï¼ˆWandbã§è¨˜éŒ²ï¼‰
- A/Bãƒ†ã‚¹ãƒˆå¯èƒ½ãªè¨­è¨ˆ

#### ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
- README.md: ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ‰‹é †
- API.md: ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆä»•æ§˜
- PROMPT.md: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé›†

---

## 7. ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### 7.1 ã‚ˆãã‚ã‚‹å•é¡Œ

#### SambaNova API
- **å•é¡Œ**: Rate limitè¶…é
- **å¯¾ç­–**: ãƒãƒƒãƒå‡¦ç†ã¨ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯å®Ÿè£…

#### vLLM
- **å•é¡Œ**: ãƒ¡ãƒ¢ãƒªä¸è¶³
- **å¯¾ç­–**: ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã‚’8Bã«å¤‰æ›´ã€é‡å­åŒ–ï¼ˆ4-bitï¼‰

#### ç²¾åº¦ä¸è¶³
- **å•é¡Œ**: ã‚¢ã‚¯ã‚·ãƒ§ãƒ³èªè­˜ç²¾åº¦ãŒä½ã„
- **å¯¾ç­–**: Few-shot examplesè¿½åŠ ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ”¹å–„

### 7.2 ç·Šæ€¥æ™‚ã®ä»£æ›¿æ¡ˆ

| å•é¡Œ | ä»£æ›¿æ¡ˆ |
|------|--------|
| vLLMãŒå‹•ã‹ãªã„ | OpenAI API (GPT-4o-mini) |
| æ‰‹è¡“å‹•ç”»ãŒå…¥æ‰‹ã§ããªã„ | YouTubeã®æ–™ç†å‹•ç”»ã§æ¦‚å¿µå®Ÿè¨¼ |
| SambaNovaãŒé…ã„ | ãƒãƒƒãƒã‚µã‚¤ã‚ºå‰Šæ¸›ã€ãƒ•ãƒ¬ãƒ¼ãƒ é–“å¼•ã |

---

## 8. ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æˆ¦ç•¥

### 8.1 ãƒ‡ãƒ¢ã‚·ãƒŠãƒªã‚ª

1. **ã‚ªãƒ¼ãƒ—ãƒ‹ãƒ³ã‚° (30ç§’)**
   - å•é¡Œæèµ·: ã€Œå¤–ç§‘åŒ»ã®æŠ€è¡“ç¶™æ‰¿å±æ©Ÿã€

2. **ãƒ‡ãƒ¢ Part 1: å³æ™‚è§£æ (60ç§’)**
   - å‹•ç”»ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
   - ã€Œè§£æä¸­...ã€â†’ 3åˆ†ã§å®Œäº†ï¼ˆé€šå¸¸30åˆ†ã®ä½œæ¥­ï¼‰
   - ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³è¡¨ç¤º

3. **ãƒ‡ãƒ¢ Part 2: ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–æ¤œç´¢ (60ç§’)**
   - ã€Œçµç´®ã€ã§æ¤œç´¢ â†’ è©²å½“ã‚·ãƒ¼ãƒ³ã¸ã‚¸ãƒ£ãƒ³ãƒ—
   - AIè§£èª¬ã®è¡¨ç¤º

4. **æŠ€è¡“èª¬æ˜ (30ç§’)**
   - SambaNova + vLLMã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ§‹æˆ
   - Wandbã§ã®ç¶™ç¶šçš„æ”¹å–„

5. **ã‚¯ãƒ­ãƒ¼ã‚¸ãƒ³ã‚° (30ç§’)**
   - ç¤¾ä¼šçš„ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ
   - ä»Šå¾Œã®å±•æœ›

### 8.2 æƒ³å®šè³ªå•ã¨å›ç­”

**Q**: å®Ÿéš›ã®ç—…é™¢ã«å°å…¥ã§ãã¾ã™ã‹ï¼Ÿ
**A**: ã¯ã„ã€‚vLLMã«ã‚ˆã‚‹ãƒ­ãƒ¼ã‚«ãƒ«æ¨è«–ã§åŒ»ç™‚æƒ…å ±ã®ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ã‚’ä¿è­·ã—ã¾ã™ã€‚SambaNovaã¯åŒ¿ååŒ–ã•ã‚ŒãŸç”»åƒã®ã¿é€ä¿¡ã—ã¾ã™ã€‚

**Q**: ä»–ã®è¨ºç™‚ç§‘ã§ã‚‚ä½¿ãˆã¾ã™ã‹ï¼Ÿ
**A**: ã¯ã„ã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã‚’å¤‰æ›´ã™ã‚‹ã ã‘ã§ã€å†…è¦–é¡æ¤œæŸ»ã‚„æ•´å½¢å¤–ç§‘æ‰‹è¡“ã«ã‚‚å¯¾å¿œå¯èƒ½ã§ã™ã€‚

**Q**: ç²¾åº¦ã¯ï¼Ÿ
**A**: Cholec80ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³èªè­˜ç²¾åº¦ã¯85%ä»¥ä¸Šã‚’é”æˆã—ã¦ã„ã¾ã™ï¼ˆWandbãƒ­ã‚°å‚ç…§ï¼‰ã€‚

---

## 9. æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆãƒãƒƒã‚«ã‚½ãƒ³å¾Œï¼‰

### Phase 1: ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—æ”¹å–„ (1-2é€±é–“)
- å®Ÿéš›ã®åŒ»å¸«ã‹ã‚‰ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åé›†
- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç²¾åº¦å‘ä¸Š
- UI/UXæ”¹å–„

### Phase 2: è‡¨åºŠè©¦é¨“ (1-3ãƒ¶æœˆ)
- ææºç—…é™¢ã§ã®ãƒ‘ã‚¤ãƒ­ãƒƒãƒˆé‹ç”¨
- æ•™è‚²åŠ¹æœã®å®šé‡è©•ä¾¡
- ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç›£æŸ»

### Phase 3: ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆåŒ– (3-6ãƒ¶æœˆ)
- ãƒãƒ«ãƒãƒ†ãƒŠãƒ³ãƒˆå¯¾å¿œ
- DICOMé€£æº
- ãƒ¢ãƒã‚¤ãƒ«ã‚¢ãƒ—ãƒªé–‹ç™º

---

## 10. å‚è€ƒè³‡æ–™

### Llamaé–¢é€£
- [Meta Llama Documentation](https://ai.meta.com/llama/)
- [SambaNova API Docs](https://sambanova.ai/docs)
- [vLLM Documentation](https://docs.vllm.ai/)

### åŒ»ç™‚AI
- [Cholec80 Dataset](http://camma.u-strasbg.fr/datasets)
- [æ—¥æœ¬å¤–ç§‘å­¦ä¼šã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³](https://www.jssoc.or.jp/)

### é–‹ç™ºãƒ„ãƒ¼ãƒ«
- [Wandb Guides](https://docs.wandb.ai/)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [Next.js Documentation](https://nextjs.org/docs)

---

**ä½œæˆæ—¥**: 2024å¹´ï¼ˆãƒãƒƒã‚«ã‚½ãƒ³é–‹å‚¬æ—¥ï¼‰
**æœ€çµ‚æ›´æ–°**: Day 1 é–‹å§‹å‰
**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.0
**ãƒ¡ãƒ³ãƒ†ãƒŠ**: Surgical-Recapé–‹ç™ºãƒãƒ¼ãƒ 

---

## Appendix: ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

```bash
# 1. ãƒªãƒã‚¸ãƒˆãƒªã‚¯ãƒ­ãƒ¼ãƒ³
git clone https://github.com/your-team/surgical-recap.git
cd surgical-recap

# 2. ç’°å¢ƒå¤‰æ•°è¨­å®š
cp .env.example .env
# SAMBANOVA_API_KEY, WANDB_API_KEYã‚’è¨­å®š

# 3. Backendèµ·å‹•
cd backend
uv sync
uv run uvicorn app.main:app --reload --port 8000

# 4. Frontendèµ·å‹•ï¼ˆåˆ¥ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ï¼‰
cd frontend
npm install
npm run dev

# 5. ãƒ–ãƒ©ã‚¦ã‚¶ã§ç¢ºèª
# Backend: http://localhost:8000/docs
# Frontend: http://localhost:3000
```

**Good luck with your hackathon! ğŸš€**
